{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GLOVE_DIR = os.path.expanduser('~\\\\data\\\\')\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "MAX_NB_WORDS = 1500\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {} # word -> coefs\n",
    "# We'll use the 100-dimensional version\n",
    "with open('glove.6B.100d.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.430</td>\n",
       "      <td>Morrisons</td>\n",
       "      <td>Morrisons book second consecutive quarter of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>IMI</td>\n",
       "      <td>IMI posts drop in first-quarter organic revenu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.340</td>\n",
       "      <td>Glencore</td>\n",
       "      <td>Glencore to refinance its short-term debt earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.259</td>\n",
       "      <td>Ryanair</td>\n",
       "      <td>EasyJet attracts more passengers in June but s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays 'bad bank' chief to step down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  sentiment SentimentSource  \\\n",
       "0        2      0.430       Morrisons   \n",
       "1        3     -0.344             IMI   \n",
       "2        4      0.340        Glencore   \n",
       "3        5      0.259         Ryanair   \n",
       "4        6     -0.231        Barclays   \n",
       "\n",
       "                                       SentimentText  \n",
       "0  Morrisons book second consecutive quarter of s...  \n",
       "1  IMI posts drop in first-quarter organic revenu...  \n",
       "2  Glencore to refinance its short-term debt earl...  \n",
       "3  EasyJet attracts more passengers in June but s...  \n",
       "4             Barclays 'bad bank' chief to step down  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= df['SentimentText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_index = defaultdict(list)\n",
    "for pos, term in enumerate(a.split()):\n",
    "    pos_index[term].append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['new_senti']=df['sentiment'].apply(lambda row: 0.0 if row <= 0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y=df['sentiment']\n",
    "y=df['new_senti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(913, 20)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=maxlen*20)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "data = pad_sequences(sequences, maxlen)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 20)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=maxlen*20)\n",
    "tokenizer.fit_on_texts(x_test)\n",
    "sequences = tokenizer.texts_to_sequences(x_test)\n",
    "data1 = pad_sequences(sequences, maxlen)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1142, 20)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer(num_words=maxlen*20)\n",
    "tokenizer1.fit_on_texts(x)\n",
    "sequences = tokenizer1.texts_to_sequences(x)\n",
    "full_data = pad_sequences(sequences, maxlen)\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a regular MLP\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(256, activation='linear')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='linear')(x)\n",
    "preds = Dense(1, activation='tanh')(x)\n",
    "\n",
    "model_mlp = Model(sequence_input, preds)\n",
    "model_mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 20, 100)           108900    \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 256)               512256    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 687,205\n",
      "Trainable params: 578,305\n",
      "Non-trainable params: 108,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 913 samples, validate on 229 samples\n",
      "Epoch 1/10\n",
      "913/913 [==============================] - 1s - loss: 0.4351 - acc: 0.5641 - val_loss: 0.3939 - val_acc: 0.6026\n",
      "Epoch 2/10\n",
      "913/913 [==============================] - 0s - loss: 0.4254 - acc: 0.5696 - val_loss: 0.3893 - val_acc: 0.6070\n",
      "Epoch 3/10\n",
      "913/913 [==============================] - 0s - loss: 0.4290 - acc: 0.5608 - val_loss: 0.4014 - val_acc: 0.5983\n",
      "Epoch 4/10\n",
      "913/913 [==============================] - 0s - loss: 0.4322 - acc: 0.5652 - val_loss: 0.4016 - val_acc: 0.5983\n",
      "Epoch 5/10\n",
      "913/913 [==============================] - 0s - loss: 0.4317 - acc: 0.5663 - val_loss: 0.4016 - val_acc: 0.5983\n",
      "Epoch 6/10\n",
      "913/913 [==============================] - 0s - loss: 0.4319 - acc: 0.5663 - val_loss: 0.4014 - val_acc: 0.5983\n",
      "Epoch 7/10\n",
      "913/913 [==============================] - 0s - loss: 0.4313 - acc: 0.5652 - val_loss: 0.3998 - val_acc: 0.5983\n",
      "Epoch 8/10\n",
      "913/913 [==============================] - 0s - loss: 0.4263 - acc: 0.5663 - val_loss: 0.3984 - val_acc: 0.5983\n",
      "Epoch 9/10\n",
      "913/913 [==============================] - 0s - loss: 0.4178 - acc: 0.5794 - val_loss: 0.4026 - val_acc: 0.5939\n",
      "Epoch 10/10\n",
      "913/913 [==============================] - 0s - loss: 0.4230 - acc: 0.5696 - val_loss: 0.3994 - val_acc: 0.5983\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    print('Training model.')\n",
    "    history = model_mlp.fit(data, y_train, validation_data=(data1, y_test),\n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.83%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model_mlp.evaluate(data1, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_size = (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='linear')(embedded_sequences)\n",
    "#x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "x = Conv1D(128, 5, activation='linear')(x)\n",
    "#x=LSTM(128, dropout=0.3, recurrent_dropout=0.15)\n",
    "#x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "x = Conv1D(128, 5, activation='linear')(x)\n",
    "#x = MaxPooling1D(pool_size=pool_size)(x)  # modified from example since our seq len is 300 \n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='linear')(x)\n",
    "preds = Dense(1, activation='tanh')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='cosine_proximity',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 20, 100)           108900    \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16, 128)           64128     \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 12, 128)           82048     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 8, 128)            82048     \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 468,453\n",
      "Trainable params: 359,553\n",
      "Non-trainable params: 108,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataa=data.reshape(-1,20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataa1=data1.reshape(-1,20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 20, 1)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataa1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 913 samples, validate on 229 samples\n",
      "Epoch 1/20\n",
      "913/913 [==============================] - 1s - loss: -0.3593 - acc: 0.4907 - val_loss: -0.5546 - val_acc: 0.5633\n",
      "Epoch 2/20\n",
      "913/913 [==============================] - 0s - loss: -0.5542 - acc: 0.5564 - val_loss: -0.5895 - val_acc: 0.5721\n",
      "Epoch 3/20\n",
      "913/913 [==============================] - 0s - loss: -0.5608 - acc: 0.5520 - val_loss: -0.5983 - val_acc: 0.5764\n",
      "Epoch 4/20\n",
      "913/913 [==============================] - 0s - loss: -0.5608 - acc: 0.5608 - val_loss: -0.5983 - val_acc: 0.5852\n",
      "Epoch 5/20\n",
      "913/913 [==============================] - 0s - loss: -0.5608 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5895\n",
      "Epoch 6/20\n",
      "913/913 [==============================] - 0s - loss: -0.5608 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5895\n",
      "Epoch 7/20\n",
      "913/913 [==============================] - 0s - loss: -0.5630 - acc: 0.5630 - val_loss: -0.5983 - val_acc: 0.5895\n",
      "Epoch 8/20\n",
      "913/913 [==============================] - 0s - loss: -0.5630 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5939\n",
      "Epoch 9/20\n",
      "913/913 [==============================] - 0s - loss: -0.5630 - acc: 0.5619 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 10/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5619 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 11/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5608 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 12/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5619 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 13/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5608 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 14/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5608 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 15/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5608 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 16/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5619 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 17/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 18/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 19/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5641 - val_loss: -0.5983 - val_acc: 0.5983\n",
      "Epoch 20/20\n",
      "913/913 [==============================] - 0s - loss: -0.5652 - acc: 0.5652 - val_loss: -0.5983 - val_acc: 0.5983\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "history = model.fit(data, y_train, validation_data=(data1, y_test),\n",
    "          epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.83%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model.evaluate(data1, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_result = model.predict(data1)\n",
    "predict_result1 = model_mlp.predict(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999958],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999994],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999905],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999642],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999964],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999952],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999976],\n",
       "       [ 1.        ],\n",
       "       [ 0.70695335],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999905],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99994063],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99997663],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99979883],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99661976],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999934],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999768],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99992925],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999994],\n",
       "       [ 0.99910557],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.98869663],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99998385],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.9999997 ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99942708],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999988],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999976],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.96869498],\n",
       "       [ 0.99999803],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.9999038 ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999988],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999946],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99996716],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999058],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999964],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.9999997 ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99943244],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99993807],\n",
       "       [ 0.99999994],\n",
       "       [ 1.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.99999976],\n",
       "       [ 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cx= predict_result.reshape(1,-1)\n",
    "cy= y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77342538]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(cx,cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units = 100\n",
    "nb_classes = 1\n",
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(embedding_layer(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')))\n",
    "model.add(LSTM(128,input_shape=(20,1)))\n",
    "#model.add(LSTM(128,output_dim=hidden_units, return_sequences =True))\n",
    "#model.add(LSTM(output_dim=hidden_units, return_sequences =False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print(\"Train...\")\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 913 samples, validate on 229 samples\n",
      "Epoch 1/20\n",
      "913/913 [==============================] - 2s - loss: nan - acc: 0.0197 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "913/913 [==============================] - 0s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "history = model.fit(dataa, y_train, validation_data=(dataa1, y_test),\n",
    "          epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
