{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code was removed by DSX for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO  \n",
    "import requests  \n",
    "import json  \n",
    "import pandas as pd\n",
    "\n",
    "def get_data(credentials):  \n",
    "    \"\"\"This functions returns a StringIO object containing\n",
    "    the file content from Bluemix Object Storage V3.\"\"\"\n",
    "\n",
    "    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n",
    "    data = {'auth': {'identity': {'methods': ['password'],\n",
    "            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n",
    "            'password': credentials['password']}}}}}\n",
    "    headers1 = {'Content-Type': 'application/json'}\n",
    "    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n",
    "    resp1_body = resp1.json()\n",
    "    for e1 in resp1_body['token']['catalog']:\n",
    "        if(e1['type']=='object-store'):\n",
    "            for e2 in e1['endpoints']:\n",
    "                        if(e2['interface']=='public'and e2['region']=='dallas'):\n",
    "                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', credentials['filename']])\n",
    "    s_subject_token = resp1.headers['x-subject-token']\n",
    "    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n",
    "    resp2 = requests.get(url=url2, headers=headers2)\n",
    "    return json.loads(resp2.content)\n",
    "\n",
    "my_json = get_data(credentials_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = my_json.get(\"batch_size\")\n",
    "epochs = my_json.get(\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", input_shape=(3, 32, 32...)`\n",
      "  app.launch_new_instance()\n",
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3))`\n",
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\")`\n",
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3))`\n",
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(48, 3, 3, border_mode='same', input_shape=(3, 32, 32,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(96, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(96, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(192, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(192, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "data_augmentation = True\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 640s - loss: 1.3095 - acc: 0.5296 - val_loss: 1.1968 - val_acc: 0.5706\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 613s - loss: 1.1952 - acc: 0.5769 - val_loss: 1.0377 - val_acc: 0.6213\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 683s - loss: 1.1052 - acc: 0.6099 - val_loss: 0.9276 - val_acc: 0.6751\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 652s - loss: 1.0400 - acc: 0.6372 - val_loss: 0.8885 - val_acc: 0.6916\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 625s - loss: 0.9995 - acc: 0.6523 - val_loss: 0.8158 - val_acc: 0.7135\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c78d09832681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Evaluate model with test data set and share sample prediction results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "\n",
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.70\n",
      "Actual Label = cat vs. Predicted Label = cat\n",
      "Actual Label = ship vs. Predicted Label = ship\n",
      "Actual Label = ship vs. Predicted Label = ship\n",
      "Actual Label = airplane vs. Predicted Label = ship\n",
      "Actual Label = frog vs. Predicted Label = frog\n",
      "Actual Label = frog vs. Predicted Label = frog\n",
      "Actual Label = automobile vs. Predicted Label = truck\n",
      "Actual Label = frog vs. Predicted Label = bird\n",
      "Actual Label = cat vs. Predicted Label = cat\n",
      "Actual Label = automobile vs. Predicted Label = truck\n",
      "Actual Label = airplane vs. Predicted Label = cat\n",
      "Actual Label = truck vs. Predicted Label = truck\n",
      "Actual Label = dog vs. Predicted Label = cat\n",
      "Actual Label = horse vs. Predicted Label = horse\n",
      "Actual Label = truck vs. Predicted Label = truck\n",
      "Actual Label = ship vs. Predicted Label = frog\n",
      "Actual Label = dog vs. Predicted Label = dog\n",
      "Actual Label = horse vs. Predicted Label = deer\n",
      "Actual Label = ship vs. Predicted Label = ship\n",
      "Actual Label = frog vs. Predicted Label = frog\n",
      "Actual Label = horse vs. Predicted Label = horse\n"
     ]
    }
   ],
   "source": [
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 11\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate the x dataset\n",
    "full_dataset_x = np.concatenate((x_train, x_test), axis=0)\n",
    "#random sampling of 12000 images\n",
    "a = np.random.randint(0,60000,size=12000)\n",
    "sample_x = full_dataset_x[a]\n",
    "#concatenate the y dataset\n",
    "full_dataset_y = np.concatenate((y_train, y_test), axis=0)\n",
    "#random sampling of 12000 \n",
    "sample_y = full_dataset_y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "#sample_x = np_utils.to_categorical(sample_x)\n",
    "sample_y = np_utils.to_categorical(sample_y)\n",
    "num_classes = sample_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 657s - loss: 0.9050 - acc: 0.6874 - val_loss: 9.5194 - val_acc: 0.4021\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 608s - loss: 0.8544 - acc: 0.7067 - val_loss: 8.0234 - val_acc: 0.4867\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 676s - loss: 0.8128 - acc: 0.7206 - val_loss: 9.2444 - val_acc: 0.4175\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 634s - loss: 0.7907 - acc: 0.7290 - val_loss: 8.8841 - val_acc: 0.4412\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 486s - loss: 0.7696 - acc: 0.7395 - val_loss: 8.8989 - val_acc: 0.4363\n",
      "Accuracy: 73.73%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(sample_x, sample_y), epochs=epochs, batch_size=batch_size)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.42\n",
      "Actual Label = bird vs. Predicted Label = truck\n",
      "Actual Label = cat vs. Predicted Label = horse\n",
      "Actual Label = horse vs. Predicted Label = horse\n",
      "Actual Label = cat vs. Predicted Label = truck\n",
      "Actual Label = horse vs. Predicted Label = horse\n",
      "Actual Label = truck vs. Predicted Label = airplane\n",
      "Actual Label = bird vs. Predicted Label = truck\n",
      "Actual Label = horse vs. Predicted Label = truck\n",
      "Actual Label = cat vs. Predicted Label = horse\n",
      "Actual Label = deer vs. Predicted Label = ship\n",
      "Actual Label = cat vs. Predicted Label = automobile\n",
      "Actual Label = bird vs. Predicted Label = ship\n",
      "Actual Label = ship vs. Predicted Label = horse\n",
      "Actual Label = cat vs. Predicted Label = airplane\n",
      "Actual Label = bird vs. Predicted Label = airplane\n",
      "Actual Label = cat vs. Predicted Label = horse\n",
      "Actual Label = frog vs. Predicted Label = horse\n",
      "Actual Label = automobile vs. Predicted Label = airplane\n",
      "Actual Label = deer vs. Predicted Label = ship\n",
      "Actual Label = airplane vs. Predicted Label = ship\n",
      "Actual Label = frog vs. Predicted Label = airplane\n"
     ]
    }
   ],
   "source": [
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(sample_x, sample_y,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=sample_x.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen1 = model.predict_generator(datagen.flow(sample_x, sample_y,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=sample_x.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen1):\n",
    "    actual_label = labels['label_names'][np.argmax(sample_y[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.99323088e-12,   7.85449713e-35,   1.28602801e-34, ...,\n",
       "          1.06914163e-01,   0.00000000e+00,   8.89191329e-01],\n",
       "       [  3.68248995e-37,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  2.46826221e-16,   1.53590787e-25,   1.20042597e-26, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.85066695e-13],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.04061059e-19,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          9.99999523e-01,   0.00000000e+00,   5.22972016e-07]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_gen1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_y = full_dataset_y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_y=[]\n",
    "for i in range(len(sample_y)):\n",
    "    actual_y.append(sample_y[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(actual_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 76s    \n"
     ]
    }
   ],
   "source": [
    "predix=model.predict_classes(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-9c0e17254532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mlabel_to_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;31m# convert yt, yp into index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_to_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(actual_y, predix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count= itemfreq(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 1213],\n",
       "       [   1, 1177],\n",
       "       [   2, 1236],\n",
       "       [   3, 1194],\n",
       "       [   4, 1211],\n",
       "       [   5, 1142],\n",
       "       [   6, 1227],\n",
       "       [   7, 1208],\n",
       "       [   8, 1176],\n",
       "       [   9, 1216]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('airplane', 1213)\n",
      "('automobile', 1177)\n",
      "('bird', 1236)\n",
      "('cat', 1194)\n",
      "('deer', 1211)\n",
      "('dog', 1142)\n",
      "('frog', 1227)\n",
      "('horse', 1208)\n",
      "('ship', 1176)\n",
      "('truck', 1216)\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,len(count)):\n",
    "    print(labels['label_names'][i],count[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 3, 32, 32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx_train, xx_test, yy_train, yy_test = train_test_split(\n",
    "    sample_x, sample_y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx_test = np.setdiff1d( np.arange(0,sample_x.shape[0]), xx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "xx_train = xx_train.astype('float32')\n",
    "xx_test = xx_test.astype('float32')\n",
    "xx_train = xx_train / 255.0\n",
    "xx_test = xx_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "yy_train = np_utils.to_categorical(yy_train)\n",
    "yy_test = np_utils.to_categorical(yy_test)\n",
    "num_classes = yy_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/5\n",
      "131/131 [==============================] - 132s - loss: 0.8426 - acc: 0.7121 - val_loss: 0.6044 - val_acc: 0.7925\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 113s - loss: 0.8480 - acc: 0.7127 - val_loss: 0.5779 - val_acc: 0.7986\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 87s - loss: 0.8098 - acc: 0.7323 - val_loss: 0.5781 - val_acc: 0.7953\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 85s - loss: 0.8113 - acc: 0.7283 - val_loss: 0.5978 - val_acc: 0.7886\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 84s - loss: 0.8097 - acc: 0.7302 - val_loss: 0.5877 - val_acc: 0.7986\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(xx_train, yy_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(xx_test, yy_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(xx_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(xx_train, yy_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=xx_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(xx_test, yy_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "\n",
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.77\n",
      "Actual Label = horse vs. Predicted Label = deer\n",
      "Actual Label = frog vs. Predicted Label = ship\n",
      "Actual Label = bird vs. Predicted Label = automobile\n",
      "Actual Label = automobile vs. Predicted Label = truck\n",
      "Actual Label = bird vs. Predicted Label = deer\n",
      "Actual Label = horse vs. Predicted Label = ship\n",
      "Actual Label = cat vs. Predicted Label = horse\n",
      "Actual Label = automobile vs. Predicted Label = horse\n",
      "Actual Label = ship vs. Predicted Label = dog\n",
      "Actual Label = ship vs. Predicted Label = automobile\n",
      "Actual Label = ship vs. Predicted Label = frog\n",
      "Actual Label = horse vs. Predicted Label = dog\n",
      "Actual Label = bird vs. Predicted Label = ship\n",
      "Actual Label = deer vs. Predicted Label = bird\n",
      "Actual Label = truck vs. Predicted Label = airplane\n",
      "Actual Label = airplane vs. Predicted Label = deer\n",
      "Actual Label = airplane vs. Predicted Label = automobile\n",
      "Actual Label = airplane vs. Predicted Label = horse\n",
      "Actual Label = horse vs. Predicted Label = horse\n",
      "Actual Label = dog vs. Predicted Label = truck\n",
      "Actual Label = truck vs. Predicted Label = airplane\n"
     ]
    }
   ],
   "source": [
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(xx_test, yy_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=xx_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen2 = model.predict_generator(datagen.flow(xx_test, yy_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=xx_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen2):\n",
    "    actual_label = labels['label_names'][np.argmax(yy_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
