{
    "metadata": {
        "language_info": {
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "#Import packages\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nK.set_image_dim_ordering('th')", 
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n"
                }
            ]
        }, 
        {
            "source": "import os\nimport pickle\nimport time\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers import Activation, Flatten, Dense, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 37, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 113, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "from io import BytesIO  \nimport requests  \nimport json  \nimport pandas as pd\n\ndef get_data(credentials):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', credentials['filename']])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return json.loads(resp2.content)\n\nmy_json = get_data(credentials_1)\n", 
            "metadata": {}, 
            "execution_count": 114, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "batch_size = my_json.get(\"batch_size\")\nepochs = my_json.get(\"epochs\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 117, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "batch_size", 
            "metadata": {}, 
            "execution_count": 116, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "64"
                    }, 
                    "metadata": {}, 
                    "execution_count": 116, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# load data\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# normalize inputs from 0-255 to 0.0-1.0\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train = x_train / 255.0\nx_test = x_test / 255.0", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# one hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Define the model\nmodel = Sequential()\nmodel.add(Conv2D(48, 3, 3, border_mode='same', input_shape=(3, 32, 32,)))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(48, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(96, 3, 3, border_mode='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(96, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(192, 3, 3, border_mode='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(192, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])", 
            "metadata": {}, 
            "execution_count": 23, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", input_shape=(3, 32, 32...)`\n  app.launch_new_instance()\n/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3))`\n/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\")`\n/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3))`\n/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3))`\n"
                }
            ]
        }, 
        {
            "source": "batch_size = 64\nnum_classes = 10\nepochs = 5\ndata_augmentation = True\nnum_predictions = 20", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 35, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "if not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\n", 
            "metadata": {}, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Using real-time data augmentation.\nEpoch 1/5\n781/781 [==============================] - 640s - loss: 1.3095 - acc: 0.5296 - val_loss: 1.1968 - val_acc: 0.5706\nEpoch 2/5\n781/781 [==============================] - 613s - loss: 1.1952 - acc: 0.5769 - val_loss: 1.0377 - val_acc: 0.6213\nEpoch 3/5\n781/781 [==============================] - 683s - loss: 1.1052 - acc: 0.6099 - val_loss: 0.9276 - val_acc: 0.6751\nEpoch 4/5\n781/781 [==============================] - 652s - loss: 1.0400 - acc: 0.6372 - val_loss: 0.8885 - val_acc: 0.6916\nEpoch 5/5\n781/781 [==============================] - 625s - loss: 0.9995 - acc: 0.6523 - val_loss: 0.8158 - val_acc: 0.7135\n"
                }, 
                {
                    "ename": "NameError", 
                    "output_type": "error", 
                    "evalue": "name 'pickle' is not defined", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-36-c78d09832681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Evaluate model with test data set and share sample prediction results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
                    ]
                }
            ]
        }, 
        {
            "source": "with open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", 
            "metadata": {}, 
            "execution_count": 38, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Model Accuracy = 0.70\nActual Label = cat vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = ship\nActual Label = ship vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = frog\nActual Label = frog vs. Predicted Label = frog\nActual Label = automobile vs. Predicted Label = truck\nActual Label = frog vs. Predicted Label = bird\nActual Label = cat vs. Predicted Label = cat\nActual Label = automobile vs. Predicted Label = truck\nActual Label = airplane vs. Predicted Label = cat\nActual Label = truck vs. Predicted Label = truck\nActual Label = dog vs. Predicted Label = cat\nActual Label = horse vs. Predicted Label = horse\nActual Label = truck vs. Predicted Label = truck\nActual Label = ship vs. Predicted Label = frog\nActual Label = dog vs. Predicted Label = dog\nActual Label = horse vs. Predicted Label = deer\nActual Label = ship vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = frog\nActual Label = horse vs. Predicted Label = horse\n"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# fix random seed for reproducibility\nseed = 11\nnp.random.seed(seed)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "#concatenate the x dataset\nfull_dataset_x = np.concatenate((x_train, x_test), axis=0)\n#random sampling of 12000 images\na = np.random.randint(0,60000,size=12000)\nsample_x = full_dataset_x[a]\n#concatenate the y dataset\nfull_dataset_y = np.concatenate((y_train, y_test), axis=0)\n#random sampling of 12000 \nsample_y = full_dataset_y[a]", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# one hot encode outputs\n#sample_x = np_utils.to_categorical(sample_x)\nsample_y = np_utils.to_categorical(sample_y)\nnum_classes = sample_y.shape[1]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 41, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Fit the model\nmodel.fit(x_train, y_train, validation_data=(sample_x, sample_y), epochs=epochs, batch_size=batch_size)\n# Final evaluation of the model\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))", 
            "metadata": {}, 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Train on 50000 samples, validate on 12000 samples\nEpoch 1/5\n50000/50000 [==============================] - 657s - loss: 0.9050 - acc: 0.6874 - val_loss: 9.5194 - val_acc: 0.4021\nEpoch 2/5\n50000/50000 [==============================] - 608s - loss: 0.8544 - acc: 0.7067 - val_loss: 8.0234 - val_acc: 0.4867\nEpoch 3/5\n50000/50000 [==============================] - 676s - loss: 0.8128 - acc: 0.7206 - val_loss: 9.2444 - val_acc: 0.4175\nEpoch 4/5\n50000/50000 [==============================] - 634s - loss: 0.7907 - acc: 0.7290 - val_loss: 8.8841 - val_acc: 0.4412\nEpoch 5/5\n50000/50000 [==============================] - 486s - loss: 0.7696 - acc: 0.7395 - val_loss: 8.8989 - val_acc: 0.4363\nAccuracy: 73.73%\n"
                }
            ]
        }, 
        {
            "source": "with open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model.evaluate_generator(datagen.flow(sample_x, sample_y,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=sample_x.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen1 = model.predict_generator(datagen.flow(sample_x, sample_y,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=sample_x.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen1):\n    actual_label = labels['label_names'][np.argmax(sample_y[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", 
            "metadata": {}, 
            "execution_count": 44, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Model Accuracy = 0.42\nActual Label = bird vs. Predicted Label = truck\nActual Label = cat vs. Predicted Label = horse\nActual Label = horse vs. Predicted Label = horse\nActual Label = cat vs. Predicted Label = truck\nActual Label = horse vs. Predicted Label = horse\nActual Label = truck vs. Predicted Label = airplane\nActual Label = bird vs. Predicted Label = truck\nActual Label = horse vs. Predicted Label = truck\nActual Label = cat vs. Predicted Label = horse\nActual Label = deer vs. Predicted Label = ship\nActual Label = cat vs. Predicted Label = automobile\nActual Label = bird vs. Predicted Label = ship\nActual Label = ship vs. Predicted Label = horse\nActual Label = cat vs. Predicted Label = airplane\nActual Label = bird vs. Predicted Label = airplane\nActual Label = cat vs. Predicted Label = horse\nActual Label = frog vs. Predicted Label = horse\nActual Label = automobile vs. Predicted Label = airplane\nActual Label = deer vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = airplane\n"
                }
            ]
        }, 
        {
            "source": "predict_gen1", 
            "metadata": {}, 
            "execution_count": 45, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[  2.99323088e-12,   7.85449713e-35,   1.28602801e-34, ...,\n          1.06914163e-01,   0.00000000e+00,   8.89191329e-01],\n       [  3.68248995e-37,   0.00000000e+00,   0.00000000e+00, ...,\n          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n       ..., \n       [  2.46826221e-16,   1.53590787e-25,   1.20042597e-26, ...,\n          0.00000000e+00,   0.00000000e+00,   6.85066695e-13],\n       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n       [  1.04061059e-19,   0.00000000e+00,   0.00000000e+00, ...,\n          9.99999523e-01,   0.00000000e+00,   5.22972016e-07]], dtype=float32)"
                    }, 
                    "metadata": {}, 
                    "execution_count": 45, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "from sklearn.metrics import confusion_matrix", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 43, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "len(predix)", 
            "metadata": {}, 
            "execution_count": 50, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "12000"
                    }, 
                    "metadata": {}, 
                    "execution_count": 50, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "sample_y = full_dataset_y[a]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 55, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "actual_y=[]\nfor i in range(len(sample_y)):\n    actual_y.append(sample_y[i].tolist())", 
            "metadata": {}, 
            "execution_count": 60, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "type(actual_y[0])", 
            "metadata": {}, 
            "execution_count": 66, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "list"
                    }, 
                    "metadata": {}, 
                    "execution_count": 66, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "predix=model.predict_classes(sample_x)", 
            "metadata": {}, 
            "execution_count": 49, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "12000/12000 [==============================] - 76s    \n"
                }
            ]
        }, 
        {
            "source": "cm = confusion_matrix(actual_y, predix, labels)", 
            "metadata": {}, 
            "execution_count": 63, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "TypeError", 
                    "output_type": "error", 
                    "evalue": "iteration over a 0-d array", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-63-9c0e17254532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mlabel_to_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;31m# convert yt, yp into index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_to_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
                    ]
                }
            ]
        }, 
        {
            "source": "from scipy.stats import itemfreq", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 107, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "count= itemfreq(sample_y)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 108, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "count", 
            "metadata": {}, 
            "execution_count": 109, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[   0, 1213],\n       [   1, 1177],\n       [   2, 1236],\n       [   3, 1194],\n       [   4, 1211],\n       [   5, 1142],\n       [   6, 1227],\n       [   7, 1208],\n       [   8, 1176],\n       [   9, 1216]])"
                    }, 
                    "metadata": {}, 
                    "execution_count": 109, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "for i in range (0,len(count)):\n    print(labels['label_names'][i],count[i][1])", 
            "metadata": {}, 
            "execution_count": 110, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "('airplane', 1213)\n('automobile', 1177)\n('bird', 1236)\n('cat', 1194)\n('deer', 1211)\n('dog', 1142)\n('frog', 1227)\n('horse', 1208)\n('ship', 1176)\n('truck', 1216)\n"
                }
            ]
        }, 
        {
            "source": "sample_x.shape[0]", 
            "metadata": {}, 
            "execution_count": 93, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "12000"
                    }, 
                    "metadata": {}, 
                    "execution_count": 93, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "xx_train.shape", 
            "metadata": {}, 
            "execution_count": 101, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "(8400, 3, 32, 32)"
                    }, 
                    "metadata": {}, 
                    "execution_count": 101, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "from sklearn.cross_validation import train_test_split", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 97, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "xx_train, xx_test, yy_train, yy_test = train_test_split(\n    sample_x, sample_y, test_size=0.30, random_state=42)", 
            "metadata": {}, 
            "execution_count": 99, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "len(xx_train)", 
            "metadata": {}, 
            "execution_count": 100, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "8400"
                    }, 
                    "metadata": {}, 
                    "execution_count": 100, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "xx_test = np.setdiff1d( np.arange(0,sample_x.shape[0]), xx_train)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 83, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "len(xx_test)", 
            "metadata": {}, 
            "execution_count": 84, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "3600"
                    }, 
                    "metadata": {}, 
                    "execution_count": 84, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# normalize inputs from 0-255 to 0.0-1.0\nxx_train = xx_train.astype('float32')\nxx_test = xx_test.astype('float32')\nxx_train = xx_train / 255.0\nxx_test = xx_test / 255.0", 
            "metadata": {}, 
            "execution_count": 103, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# one hot encode outputs\nyy_train = np_utils.to_categorical(yy_train)\nyy_test = np_utils.to_categorical(yy_test)\nnum_classes = yy_test.shape[1]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 104, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "if not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(xx_train, yy_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(xx_test, yy_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(xx_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(xx_train, yy_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=xx_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(xx_test, yy_test),\n                        workers=4)\n\n\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\n", 
            "metadata": {}, 
            "execution_count": 105, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Using real-time data augmentation.\nEpoch 1/5\n131/131 [==============================] - 132s - loss: 0.8426 - acc: 0.7121 - val_loss: 0.6044 - val_acc: 0.7925\nEpoch 2/5\n131/131 [==============================] - 113s - loss: 0.8480 - acc: 0.7127 - val_loss: 0.5779 - val_acc: 0.7986\nEpoch 3/5\n131/131 [==============================] - 87s - loss: 0.8098 - acc: 0.7323 - val_loss: 0.5781 - val_acc: 0.7953\nEpoch 4/5\n131/131 [==============================] - 85s - loss: 0.8113 - acc: 0.7283 - val_loss: 0.5978 - val_acc: 0.7886\nEpoch 5/5\n131/131 [==============================] - 84s - loss: 0.8097 - acc: 0.7302 - val_loss: 0.5877 - val_acc: 0.7986\n"
                }
            ]
        }, 
        {
            "source": "with open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model.evaluate_generator(datagen.flow(xx_test, yy_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=xx_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen2 = model.predict_generator(datagen.flow(xx_test, yy_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=xx_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen2):\n    actual_label = labels['label_names'][np.argmax(yy_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", 
            "metadata": {}, 
            "execution_count": 106, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Model Accuracy = 0.77\nActual Label = horse vs. Predicted Label = deer\nActual Label = frog vs. Predicted Label = ship\nActual Label = bird vs. Predicted Label = automobile\nActual Label = automobile vs. Predicted Label = truck\nActual Label = bird vs. Predicted Label = deer\nActual Label = horse vs. Predicted Label = ship\nActual Label = cat vs. Predicted Label = horse\nActual Label = automobile vs. Predicted Label = horse\nActual Label = ship vs. Predicted Label = dog\nActual Label = ship vs. Predicted Label = automobile\nActual Label = ship vs. Predicted Label = frog\nActual Label = horse vs. Predicted Label = dog\nActual Label = bird vs. Predicted Label = ship\nActual Label = deer vs. Predicted Label = bird\nActual Label = truck vs. Predicted Label = airplane\nActual Label = airplane vs. Predicted Label = deer\nActual Label = airplane vs. Predicted Label = automobile\nActual Label = airplane vs. Predicted Label = horse\nActual Label = horse vs. Predicted Label = horse\nActual Label = dog vs. Predicted Label = truck\nActual Label = truck vs. Predicted Label = airplane\n"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}